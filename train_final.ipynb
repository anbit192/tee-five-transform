{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/processed/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset1 = Dataset.from_pandas(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Summary', 'Text'],\n",
      "    num_rows: 800\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Summary', 'Text'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "full_dataset = dataset1.train_test_split(test_size=0.2, shuffle=True)\n",
    "dataset_train = full_dataset['train']\n",
    "dataset_valid = full_dataset['test']\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 't5-small'\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 5\n",
    "OUT_DIR = './result666'\n",
    "MAX_LENGTH = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer('models/trained_spiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3022524b08e047f485588e432f9463ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bacca6863449f899a1faa68db30bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    "tokenizer = T5Tokenizer('models/trained_spiece.model')\n",
    "# Function to convert text data into model inputs and targets\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = [f\"summarize: {text}\" for text in examples['Text']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    targets = [summary for summary in examples['Summary']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Create a partial function with the tokenizer\n",
    "preprocess_function_with_tokenizer = partial(preprocess_function, tokenizer=tokenizer)\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60,506,624 total parameters.\n",
      "60,506,624 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    " \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    " \n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13e569974ec446c9c5ef7cb4aabf0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 19.1766, 'grad_norm': 143.90223693847656, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.05}\n",
      "{'loss': 17.9078, 'grad_norm': 139.80279541015625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 17.4335, 'grad_norm': 184.06051635742188, 'learning_rate': 6e-06, 'epoch': 0.15}\n",
      "{'loss': 15.7727, 'grad_norm': 108.15489196777344, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 14.0783, 'grad_norm': 109.02210235595703, 'learning_rate': 1e-05, 'epoch': 0.25}\n",
      "{'loss': 12.5055, 'grad_norm': 98.36102294921875, 'learning_rate': 1.2e-05, 'epoch': 0.3}\n",
      "{'loss': 10.332, 'grad_norm': 93.169921875, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.35}\n",
      "{'loss': 8.4961, 'grad_norm': 830.5044555664062, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 5.3842, 'grad_norm': 72.83383178710938, 'learning_rate': 1.8e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2576, 'grad_norm': 60.221187591552734, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 1.9472, 'grad_norm': 17.345600128173828, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.55}\n",
      "{'loss': 1.5973, 'grad_norm': 7.062865734100342, 'learning_rate': 2.4e-05, 'epoch': 0.6}\n",
      "{'loss': 1.2018, 'grad_norm': 4.849114418029785, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9994, 'grad_norm': 6.342186450958252, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8171, 'grad_norm': 3.3581416606903076, 'learning_rate': 3e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6483, 'grad_norm': 0.730644166469574, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6226, 'grad_norm': 1.0922362804412842, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.85}\n",
      "{'loss': 0.62, 'grad_norm': 0.591720700263977, 'learning_rate': 3.6e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5621, 'grad_norm': 0.34330445528030396, 'learning_rate': 3.8e-05, 'epoch': 0.95}\n",
      "{'loss': 0.518, 'grad_norm': 0.5117079615592957, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8a83fb81b94b4c9d9fe3429f5b6af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5537384748458862, 'eval_rouge1': 0.4595, 'eval_rouge2': 0.1798, 'eval_rougeL': 0.3448, 'eval_gen_len': 38.305, 'eval_runtime': 20.832, 'eval_samples_per_second': 9.601, 'eval_steps_per_second': 2.4, 'epoch': 1.0}\n",
      "{'loss': 0.5991, 'grad_norm': 0.38639014959335327, 'learning_rate': 4.2e-05, 'epoch': 1.05}\n",
      "{'loss': 0.5688, 'grad_norm': 0.5554342269897461, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.1}\n",
      "{'loss': 0.5192, 'grad_norm': 0.42708340287208557, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.15}\n",
      "{'loss': 0.531, 'grad_norm': 0.4686841368675232, 'learning_rate': 4.8e-05, 'epoch': 1.2}\n",
      "{'loss': 0.5264, 'grad_norm': 0.645159125328064, 'learning_rate': 5e-05, 'epoch': 1.25}\n",
      "{'loss': 0.5305, 'grad_norm': 0.6609600782394409, 'learning_rate': 5.2000000000000004e-05, 'epoch': 1.3}\n",
      "{'loss': 0.5193, 'grad_norm': 0.3591311573982239, 'learning_rate': 5.4000000000000005e-05, 'epoch': 1.35}\n",
      "{'loss': 0.5415, 'grad_norm': 0.5596582293510437, 'learning_rate': 5.6000000000000006e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5161, 'grad_norm': 5.0562424659729, 'learning_rate': 5.8e-05, 'epoch': 1.45}\n",
      "{'loss': 0.5291, 'grad_norm': 0.5405703186988831, 'learning_rate': 6e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5183, 'grad_norm': 0.5521650314331055, 'learning_rate': 6.2e-05, 'epoch': 1.55}\n",
      "{'loss': 0.527, 'grad_norm': 0.4329564869403839, 'learning_rate': 6.400000000000001e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4636, 'grad_norm': 0.4729790985584259, 'learning_rate': 6.6e-05, 'epoch': 1.65}\n",
      "{'loss': 0.4824, 'grad_norm': 0.47082340717315674, 'learning_rate': 6.800000000000001e-05, 'epoch': 1.7}\n",
      "{'loss': 0.5531, 'grad_norm': 0.39778441190719604, 'learning_rate': 7e-05, 'epoch': 1.75}\n",
      "{'loss': 0.5126, 'grad_norm': 0.47055304050445557, 'learning_rate': 7.2e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5125, 'grad_norm': 0.3866134583950043, 'learning_rate': 7.4e-05, 'epoch': 1.85}\n",
      "{'loss': 0.4776, 'grad_norm': 0.33316799998283386, 'learning_rate': 7.6e-05, 'epoch': 1.9}\n",
      "{'loss': 0.5061, 'grad_norm': 0.5113077759742737, 'learning_rate': 7.800000000000001e-05, 'epoch': 1.95}\n",
      "{'loss': 0.4907, 'grad_norm': 0.3523001968860626, 'learning_rate': 8e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67495f0027414442b0e29b5615de9f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48461928963661194, 'eval_rouge1': 0.5241, 'eval_rouge2': 0.2075, 'eval_rougeL': 0.3784, 'eval_gen_len': 38.89, 'eval_runtime': 20.5269, 'eval_samples_per_second': 9.743, 'eval_steps_per_second': 2.436, 'epoch': 2.0}\n",
      "{'loss': 0.4637, 'grad_norm': 0.266410231590271, 'learning_rate': 8.2e-05, 'epoch': 2.05}\n",
      "{'loss': 0.4554, 'grad_norm': 0.509789228439331, 'learning_rate': 8.4e-05, 'epoch': 2.1}\n",
      "{'loss': 0.4985, 'grad_norm': 0.4446052312850952, 'learning_rate': 8.6e-05, 'epoch': 2.15}\n",
      "{'loss': 0.5237, 'grad_norm': 0.350738525390625, 'learning_rate': 8.800000000000001e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4771, 'grad_norm': 0.3166179656982422, 'learning_rate': 9e-05, 'epoch': 2.25}\n",
      "{'loss': 0.4764, 'grad_norm': 0.35819077491760254, 'learning_rate': 9.200000000000001e-05, 'epoch': 2.3}\n",
      "{'loss': 0.5113, 'grad_norm': 0.37543168663978577, 'learning_rate': 9.4e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4568, 'grad_norm': 0.44727596640586853, 'learning_rate': 9.6e-05, 'epoch': 2.4}\n",
      "{'loss': 0.4954, 'grad_norm': 0.3447594940662384, 'learning_rate': 9.8e-05, 'epoch': 2.45}\n",
      "{'loss': 0.5068, 'grad_norm': 0.3583694100379944, 'learning_rate': 0.0001, 'epoch': 2.5}\n",
      "{'loss': 0.4715, 'grad_norm': 0.2740509808063507, 'learning_rate': 9.8e-05, 'epoch': 2.55}\n",
      "{'loss': 0.4972, 'grad_norm': 0.26573383808135986, 'learning_rate': 9.6e-05, 'epoch': 2.6}\n",
      "{'loss': 0.4753, 'grad_norm': 0.4178929626941681, 'learning_rate': 9.4e-05, 'epoch': 2.65}\n",
      "{'loss': 0.4458, 'grad_norm': 0.28536587953567505, 'learning_rate': 9.200000000000001e-05, 'epoch': 2.7}\n",
      "{'loss': 0.45, 'grad_norm': 0.4150052070617676, 'learning_rate': 9e-05, 'epoch': 2.75}\n",
      "{'loss': 0.4911, 'grad_norm': 0.3422919809818268, 'learning_rate': 8.800000000000001e-05, 'epoch': 2.8}\n",
      "{'loss': 0.4496, 'grad_norm': 0.28555402159690857, 'learning_rate': 8.6e-05, 'epoch': 2.85}\n",
      "{'loss': 0.4524, 'grad_norm': 0.26845672726631165, 'learning_rate': 8.4e-05, 'epoch': 2.9}\n",
      "{'loss': 0.4451, 'grad_norm': 0.2746482789516449, 'learning_rate': 8.2e-05, 'epoch': 2.95}\n",
      "{'loss': 0.4678, 'grad_norm': 0.24866899847984314, 'learning_rate': 8e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d668ebefa94e4767897a0b568e76ece7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47073325514793396, 'eval_rouge1': 0.5351, 'eval_rouge2': 0.2144, 'eval_rougeL': 0.3876, 'eval_gen_len': 39.585, 'eval_runtime': 20.9352, 'eval_samples_per_second': 9.553, 'eval_steps_per_second': 2.388, 'epoch': 3.0}\n",
      "{'loss': 0.4531, 'grad_norm': 0.28173404932022095, 'learning_rate': 7.800000000000001e-05, 'epoch': 3.05}\n",
      "{'loss': 0.4364, 'grad_norm': 0.285885751247406, 'learning_rate': 7.6e-05, 'epoch': 3.1}\n",
      "{'loss': 0.4398, 'grad_norm': 0.3147886395454407, 'learning_rate': 7.4e-05, 'epoch': 3.15}\n",
      "{'loss': 0.473, 'grad_norm': 0.47332173585891724, 'learning_rate': 7.2e-05, 'epoch': 3.2}\n",
      "{'loss': 0.4622, 'grad_norm': 0.3438221216201782, 'learning_rate': 7e-05, 'epoch': 3.25}\n",
      "{'loss': 0.4199, 'grad_norm': 0.3423343598842621, 'learning_rate': 6.800000000000001e-05, 'epoch': 3.3}\n",
      "{'loss': 0.502, 'grad_norm': 0.33004891872406006, 'learning_rate': 6.6e-05, 'epoch': 3.35}\n",
      "{'loss': 0.4771, 'grad_norm': 0.336498498916626, 'learning_rate': 6.400000000000001e-05, 'epoch': 3.4}\n",
      "{'loss': 0.4205, 'grad_norm': 0.4049893617630005, 'learning_rate': 6.2e-05, 'epoch': 3.45}\n",
      "{'loss': 0.4164, 'grad_norm': 0.4851333200931549, 'learning_rate': 6e-05, 'epoch': 3.5}\n",
      "{'loss': 0.4504, 'grad_norm': 0.31147870421409607, 'learning_rate': 5.8e-05, 'epoch': 3.55}\n",
      "{'loss': 0.5043, 'grad_norm': 0.5670795440673828, 'learning_rate': 5.6000000000000006e-05, 'epoch': 3.6}\n",
      "{'loss': 0.4753, 'grad_norm': 0.3873961269855499, 'learning_rate': 5.4000000000000005e-05, 'epoch': 3.65}\n",
      "{'loss': 0.4731, 'grad_norm': 0.2430824339389801, 'learning_rate': 5.2000000000000004e-05, 'epoch': 3.7}\n",
      "{'loss': 0.4746, 'grad_norm': 0.3920581638813019, 'learning_rate': 5e-05, 'epoch': 3.75}\n",
      "{'loss': 0.5128, 'grad_norm': 0.3329890966415405, 'learning_rate': 4.8e-05, 'epoch': 3.8}\n",
      "{'loss': 0.4966, 'grad_norm': 0.29413309693336487, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.85}\n",
      "{'loss': 0.44, 'grad_norm': 0.798331081867218, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.9}\n",
      "{'loss': 0.3874, 'grad_norm': 0.2972564399242401, 'learning_rate': 4.2e-05, 'epoch': 3.95}\n",
      "{'loss': 0.5073, 'grad_norm': 0.42847737669944763, 'learning_rate': 4e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03e58d46621427c88c9d3cf4029d4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47123128175735474, 'eval_rouge1': 0.552, 'eval_rouge2': 0.2215, 'eval_rougeL': 0.3969, 'eval_gen_len': 39.635, 'eval_runtime': 21.3552, 'eval_samples_per_second': 9.365, 'eval_steps_per_second': 2.341, 'epoch': 4.0}\n",
      "{'loss': 0.4906, 'grad_norm': 0.2612118124961853, 'learning_rate': 3.8e-05, 'epoch': 4.05}\n",
      "{'loss': 0.4372, 'grad_norm': 0.3074450194835663, 'learning_rate': 3.6e-05, 'epoch': 4.1}\n",
      "{'loss': 0.4622, 'grad_norm': 0.3396832346916199, 'learning_rate': 3.4000000000000007e-05, 'epoch': 4.15}\n",
      "{'loss': 0.4554, 'grad_norm': 0.2993573844432831, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.2}\n",
      "{'loss': 0.4571, 'grad_norm': 0.4288477599620819, 'learning_rate': 3e-05, 'epoch': 4.25}\n",
      "{'loss': 0.4687, 'grad_norm': 0.2666241526603699, 'learning_rate': 2.8000000000000003e-05, 'epoch': 4.3}\n",
      "{'loss': 0.4192, 'grad_norm': 0.41558536887168884, 'learning_rate': 2.6000000000000002e-05, 'epoch': 4.35}\n",
      "{'loss': 0.4995, 'grad_norm': 0.38977375626564026, 'learning_rate': 2.4e-05, 'epoch': 4.4}\n",
      "{'loss': 0.4184, 'grad_norm': 0.5707634091377258, 'learning_rate': 2.2000000000000003e-05, 'epoch': 4.45}\n",
      "{'loss': 0.5048, 'grad_norm': 0.37974387407302856, 'learning_rate': 2e-05, 'epoch': 4.5}\n",
      "{'loss': 0.4269, 'grad_norm': 0.29327481985092163, 'learning_rate': 1.8e-05, 'epoch': 4.55}\n",
      "{'loss': 0.4529, 'grad_norm': 0.2997646629810333, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.6}\n",
      "{'loss': 0.4624, 'grad_norm': 0.3595736026763916, 'learning_rate': 1.4000000000000001e-05, 'epoch': 4.65}\n",
      "{'loss': 0.4699, 'grad_norm': 0.2599736452102661, 'learning_rate': 1.2e-05, 'epoch': 4.7}\n",
      "{'loss': 0.4087, 'grad_norm': 0.32998254895210266, 'learning_rate': 1e-05, 'epoch': 4.75}\n",
      "{'loss': 0.4362, 'grad_norm': 0.4179529547691345, 'learning_rate': 8.000000000000001e-06, 'epoch': 4.8}\n",
      "{'loss': 0.472, 'grad_norm': 0.3170212507247925, 'learning_rate': 6e-06, 'epoch': 4.85}\n",
      "{'loss': 0.4261, 'grad_norm': 0.27156344056129456, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.9}\n",
      "{'loss': 0.4578, 'grad_norm': 0.4848077893257141, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.95}\n",
      "{'loss': 0.449, 'grad_norm': 0.3036443889141083, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8623a012e24328baf5368c21f40410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46943679451942444, 'eval_rouge1': 0.5524, 'eval_rouge2': 0.2214, 'eval_rougeL': 0.3955, 'eval_gen_len': 39.665, 'eval_runtime': 20.8228, 'eval_samples_per_second': 9.605, 'eval_steps_per_second': 2.401, 'epoch': 5.0}\n",
      "{'train_runtime': 377.3843, 'train_samples_per_second': 10.599, 'train_steps_per_second': 2.65, 'train_loss': 1.7211099138259889, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{OUT_DIR}/checkpoint-1000\"  # the path where you saved your model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer('models/trained_spiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=200,\n",
    "        num_beams=num_beams,\n",
    "        # early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khu vực Nam Bộ có nắng nóng và nắng nóng gay gắt trong các ngày từ 1-8/5; sau đó từ 9/5, nắng nóng chủ yếu tập trung tại các tỉnh miền Đông Nam Bộ. Trên cả nước đã ghi nhận nhiều trạm khí tượng xảy ra giá trị nhiệt độ cao nhất ngày vượt giá trị lịch sử. Cụ thể, cùng trong ngày 1/5, ở Đông Hà (Quảng Trị) đạt đến 43,2 độ, vượt mức 42,3 độ năm 2023; Huế (Thừa Thiên Huế) 42,1 độ, vượt kỷ lục 41,3 độ năm 1983; Đà Nẵng 41,5, vượt mức 40,5 năm 1983 hay Thủ Dầu Một (Bình Dương) 38,9 độ ngày 2/5, vượt mức 38,7 năm 2016,…\n"
     ]
    }
   ],
   "source": [
    "text = 'Khu vực Nam Bộ có nắng nóng và nắng nóng gay gắt trong các ngày từ 1-8/5; sau đó từ 9/5, nắng nóng chủ yếu tập trung tại các tỉnh miền Đông Nam Bộ. Trên cả nước đã ghi nhận nhiều trạm khí tượng xảy ra giá trị nhiệt độ cao nhất ngày vượt giá trị lịch sử. Cụ thể, cùng trong ngày 1/5, ở Đông Hà (Quảng Trị) đạt đến 43,2 độ, vượt mức 42,3 độ năm 2023; Huế (Thừa Thiên Huế) 42,1 độ, vượt kỷ lục 41,3 độ năm 1983; Đà Nẵng 41,5, vượt mức 40,5 năm 1983 hay Thủ Dầu Một (Bình Dương) 38,9 độ ngày 2/5, vượt mức 38,7 năm 2016,…'\n",
    "summary = dataset['Summary'][50000]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinh tế Trung Quốc tăng tốc trong khi các tin tức về vaccine không đủ kéo châu Âu và Mỹ phục hồi nhanh hơn.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nắng nóng chủ yếu tập trung tại các tỉnh miên Đông Nam Bộ. Trên cả nước đã ghi nhận nhiêu trạm khí tượng xảy ra giá trị nhiệt độ cao nhất ngày từ 1-8/5; sau đó từ 9/5, nắng nóng chủ yếu tập trung tại các tỉnh miên Đông Nam Bộ.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 377.3843,\n",
       " 'train_samples_per_second': 10.599,\n",
       " 'train_steps_per_second': 2.65,\n",
       " 'total_flos': 541367205888000.0,\n",
       " 'train_loss': 1.7211099138259889,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
